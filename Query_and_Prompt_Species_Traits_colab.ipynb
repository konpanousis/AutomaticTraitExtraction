{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install python-dotenv"
      ],
      "metadata": {
        "id": "Sffp4W8iD9At"
      },
      "id": "Sffp4W8iD9At",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87b5774-7742-448d-8f6a-dfd0543f052c",
      "metadata": {
        "id": "a87b5774-7742-448d-8f6a-dfd0543f052c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from json.decoder import JSONDecodeError\n",
        "\n",
        "# set a fixed seed\n",
        "import random\n",
        "random.seed(333)\n",
        "\n",
        "# the dotenv contains all the credentials for queries and prompts\n",
        "#from dotenv import load_dotenv\n",
        "#load_dotenv('./.env')\n",
        "# alternatively to avoid loading files. WARNING: REMOVE YOUR CREDENTIALS WHEN SHARING\n",
        "os.environ['GOOGLE_API'] = ''\n",
        "os.environ['GOOGLE_CSE'] = ''\n",
        "os.environ['MISTRAL_API_KEY'] = ''\n",
        "\n",
        "\n",
        "base_folder = 'Data'\n",
        "dataset_name = 'example'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e309fdb",
      "metadata": {
        "id": "9e309fdb"
      },
      "source": [
        "## Set up the species, traits and trait values of interest\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb787a01-f9ff-417f-ab28-ed68dc35b2f1",
      "metadata": {
        "id": "eb787a01-f9ff-417f-ab28-ed68dc35b2f1"
      },
      "outputs": [],
      "source": [
        "# A short example of how to input the species and trait/trait-value vocabulary\n",
        "species_to_query = ['Ceroxylon peruvianum', 'Calamus australis']\n",
        "\n",
        "traits_dict =  {\n",
        "    'Fruit Colour': ['black', 'blue', 'brown', 'green', 'grey', 'ivory', 'orange', 'pink', 'purple', 'red', 'white', 'yellow'],\n",
        "    'Flower Colour': ['black', 'blue', 'brown', 'green', 'grey', 'ivory', 'orange', 'pink', 'purple', 'red', 'white', 'yellow'],\n",
        "    'Crown layer': ['both', 'canopy', 'understorey'],\n",
        "    'Fruit Size': ['large', 'small'],\n",
        "    'Fruit Shape': ['ellipsoid', 'elongate', 'fusiform', 'globose', 'ovoid', 'pyramidal', 'rounded'],\n",
        "    'Conspicuousness': ['conspicuous', 'cryptic']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6623f17e-78c4-4fc3-9c0b-c3d96213a5bf",
      "metadata": {
        "id": "6623f17e-78c4-4fc3-9c0b-c3d96213a5bf"
      },
      "source": [
        "## Custom Search Engine Setup\n",
        "---\n",
        "\n",
        "To retrieve the corresponding URLs for the species that we are goind to check, we are using the Google Custom Search API. To this end, we must first assign the appropriate credentials and then construct a function to define a custom service engine. See more about the Custom Search API and how to set it up here: [Custom Search API](https://developers.google.com/custom-search/v1/overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959333a9-997b-4e7b-815a-2cdc2ef5a708",
      "metadata": {
        "id": "959333a9-997b-4e7b-815a-2cdc2ef5a708"
      },
      "outputs": [],
      "source": [
        "# Set up the google API credentials\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "google_api_key = os.environ['GOOGLE_API']\n",
        "cse_id = os.environ['GOOGLE_CSE']\n",
        "\n",
        "# create a search query\n",
        "def google_search(exact_term, other_search_term, api_key, cse_id, **kwargs):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Creates a search link for the custom Google search.\n",
        "    \"\"\"\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(exactTerms=exact_term, q=other_search_term, cx=cse_id, **kwargs).execute()\n",
        "    return res['items']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "405f4fe6-7806-405d-baa0-eed07faba354",
      "metadata": {
        "id": "405f4fe6-7806-405d-baa0-eed07faba354"
      },
      "source": [
        "## Query the Custom Search Engine\n",
        "---\n",
        "\n",
        "In the following, we define the search terms that we will search along with the names of the species defined before. Then, we utilize the **google_search** function defined in the previous blocks to retrieve the results from the custom search engine.\n",
        "\n",
        "Throughout the notebook, we store all the intermediate data that emerge using the *pickle* python package. In the following, we are storing the richer retrieved urls that include the site title and other information (*species_urls_full*) and just the urls (*species_urls*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10fdfa09-ef6c-4a78-b489-f3bb350556b0",
      "metadata": {
        "id": "10fdfa09-ef6c-4a78-b489-f3bb350556b0"
      },
      "outputs": [],
      "source": [
        "# the search terms to use along with the selected species\n",
        "search_terms = [\"description\",\"characteristics\"]\n",
        "\n",
        "# save the retrieved urls and links\n",
        "species_urls = collections.defaultdict(list)\n",
        "species_urls_full = collections.defaultdict(list)\n",
        "\n",
        "# parse all the species and search terms\n",
        "for species in tqdm(species_to_query):\n",
        "    for search_term in search_terms:\n",
        "        # constructs the query, e.g., Archontophoenix maxima description.\n",
        "        #query = F'{species} {search_term}'\n",
        "        exact_term = F'{species}'\n",
        "        other_search_term = F'{search_term}'\n",
        "\n",
        "        # Search results\n",
        "        search_results = google_search(exact_term, other_search_term, api_key=google_api_key, cse_id=cse_id)\n",
        "\n",
        "        # Record the google Urls\n",
        "        species_urls_full[species] = search_results\n",
        "\n",
        "        # Recoerd just the links\n",
        "        for result in search_results:\n",
        "            species_urls[species].append(result['link'])\n",
        "\n",
        "\n",
        "#save the results for all the species to files\n",
        "path_to_save = f'{base_folder}/Search_Query_Results/{dataset_name}_dataset/'\n",
        "\n",
        "#create the folder if it does not exist\n",
        "os.makedirs(path_to_save, exist_ok = True)\n",
        "\n",
        "# specific paths for urls and links\n",
        "url_path = f'{path_to_save}urls.pkl'\n",
        "full_url_path = f'{path_to_save}full_urls.pkl'\n",
        "\n",
        "# now save the data to respective files\n",
        "with open(url_path, 'wb') as f:\n",
        "    pickle.dump(species_urls, f)\n",
        "\n",
        "with open(full_url_path, 'wb') as f:\n",
        "    pickle.dump(species_urls_full, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65dc904-3305-40aa-95ae-c6a7f5b90d9a",
      "metadata": {
        "id": "b65dc904-3305-40aa-95ae-c6a7f5b90d9a"
      },
      "source": [
        "### URL Processing: Converting results to plain text\n",
        "---\n",
        "\n",
        "We need to extract the text from the links in order to be able to process them and use them for prompting. We use the *requests* and *bs4* packages to this end in the following snippet to define a function to extract paragraphs from urls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffe23fc-24e6-4cb6-9d09-17ca43b5ff86",
      "metadata": {
        "id": "6ffe23fc-24e6-4cb6-9d09-17ca43b5ff86"
      },
      "outputs": [],
      "source": [
        "# for parsing links\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_paragraphs_from_urls(url_dict):\n",
        "\n",
        "    # Initialise the dictionary that will contain the plain text for each species\n",
        "    text_dict = collections.defaultdict(list)\n",
        "\n",
        "    # Loop over species and URLS to extract the text\n",
        "    for (species, urls) in tqdm(url_dict.items(), desc=\"Species\", leave=True, position=0):\n",
        "        for url in tqdm(urls, desc=f\"URLs Species: {species}\", leave=False, position=0):\n",
        "\n",
        "            try:\n",
        "                session = requests.Session()\n",
        "                retry = Retry(total=1,\n",
        "                              connect=1,\n",
        "                              backoff_factor=0.5)\n",
        "                adapter = HTTPAdapter(max_retries=retry)\n",
        "                session.mount('http://', adapter)\n",
        "                session.mount('https://', adapter)\n",
        "                response = session.get(url, timeout=3)\n",
        "\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                paragraphs = soup.find_all('p')\n",
        "                for paragraph in paragraphs:\n",
        "                    text_dict[species].append(paragraph.text)\n",
        "\n",
        "            except:\n",
        "                text_dict[species].append(\"Invalid URL\")\n",
        "\n",
        "    return text_dict\n",
        "\n",
        "# call the extract paragraph function and save the results\n",
        "species_text = extract_paragraphs_from_urls(species_urls)\n",
        "\n",
        "with open(F\"{path_to_save}paragraphs.pkl\", 'wb') as f:\n",
        "    pickle.dump(species_text, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6558466-45c5-478b-a7bc-340b7bbf812c",
      "metadata": {
        "id": "a6558466-45c5-478b-a7bc-340b7bbf812c"
      },
      "outputs": [],
      "source": [
        "##### print some of the paragraphs to see if everything is ok.\n",
        "species_text[species_to_query[-1]][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d069f6-e34a-4a2e-9613-75334a45b6f6",
      "metadata": {
        "id": "b2d069f6-e34a-4a2e-9613-75334a45b6f6"
      },
      "source": [
        "### Text Processing: Filter and Clean\n",
        "---\n",
        "\n",
        "The extracted paragraphs are highly \"noisy\": they contain special and irrelevant characters, while some others cannot be encoded correctly or they are repeated, e.g., whitespace characters. The following snippet used the *re* package along with some regular expressions to clean the text. After cleaning is performed, we remove any potential duplicate entries (by converting the list of paragraphs to a python set) and the return the valid pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc315595-46c5-47d9-983c-3a1526957684",
      "metadata": {
        "id": "fc315595-46c5-47d9-983c-3a1526957684"
      },
      "outputs": [],
      "source": [
        "def regex_cleaner(string):\n",
        "    # Define a list of regular expression patterns and their replacements\n",
        "    cleaners = [\n",
        "        # Replace multiple consecutive whitespace characters (spaces, tabs, newlines) with a single space character\n",
        "        (\"\\s+\", \" \"),\n",
        "        # Replace multiple consecutive newline characters with a single newline character\n",
        "        (\"\\n+\", \"\\n\"),\n",
        "        # Replace multiple consecutive tab characters with a single tab character\n",
        "        (\"\\t+\", \"\\t\"),\n",
        "        # remove non-alphanumeric characters\n",
        "        (r'[^\\w\\s/]', ''),\n",
        "        # replace malformed characters,\n",
        "        ('Â',''),\n",
        "        ('â€“', '-'),\n",
        "        ('·','.'),\n",
        "        ('Ã','x'),\n",
        "        (u'\\xa0', u' '),\n",
        "        ('â€‰', ''),\n",
        "        ('â€', '-'),\n",
        "        ('x©', 'e'),\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Apply each regular expression pattern and its replacement to the input string\n",
        "    for (cleaner, replacement) in cleaners:\n",
        "        string = re.sub(cleaner, replacement, string)\n",
        "\n",
        "    # Return the cleaned string\n",
        "    return string.strip()\n",
        "\n",
        "def filter_species_dict(text_dict):\n",
        "    \"\"\"\n",
        "    Filters the descriptions in a dictionary of species, removing invalid text and duplicates.\n",
        "\n",
        "    Args:\n",
        "        text_dict (dict): A dictionary where the keys are the species names and the values are lists of text descriptions.\n",
        "\n",
        "    Returns:\n",
        "        dict: A filtered dictionary where the keys are the species names and the values are lists of valid and unique text descriptions.\n",
        "    \"\"\"\n",
        "\n",
        "    valid_species_dict = {}\n",
        "\n",
        "    # Loop through each species and its descriptions in the dictionary\n",
        "    for idx, (species, descriptions) in enumerate(tqdm(text_dict.items(), leave=False, position=0)):\n",
        "        # Create a progress bar for the species\n",
        "        species_description = f\"{idx} {species}\"\n",
        "        species_pbar = tqdm(descriptions, leave=False, position=1, desc=species_description)\n",
        "\n",
        "        valid_descriptions = []\n",
        "        # Loop through each description for the species\n",
        "        for description in species_pbar:\n",
        "            # Clean the description using regex_cleaner\n",
        "            cleaned_description = regex_cleaner(description)\n",
        "\n",
        "            if len(cleaned_description) < 10000 and len(cleaned_description) > 1:\n",
        "                valid_descriptions.append(cleaned_description)\n",
        "\n",
        "        # Remove any duplicate descriptions in the list\n",
        "        valid_descriptions = list(set(valid_descriptions))\n",
        "        # Add the valid descriptions for the species to the valid_species_dict\n",
        "        valid_species_dict[species] = valid_descriptions\n",
        "\n",
        "    return valid_species_dict\n",
        "\n",
        "# now clean the text extracted from the urls and save the results\n",
        "species_text_cleaned = filter_species_dict(species_text)\n",
        "\n",
        "with open(f\"{path_to_save}paragraphs_cleaned.pkl\", 'wb') as f:\n",
        "    pickle.dump(species_text_cleaned, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b814e1a3-b73e-44a8-b763-a9397bfad971",
      "metadata": {
        "id": "b814e1a3-b73e-44a8-b763-a9397bfad971"
      },
      "source": [
        "### Text Processing: Cleaned Text to Descriptions Classification\n",
        "---\n",
        "\n",
        "We now have a useful list of texts for each species from the extracted information from the web. However, it is highly likely that most of the extracted and cleaned parts do not contain any useful information for the respective species. To this end, we trained a Description Classifier; this model takes as input a piece of text and classifies the text as being a description or not. We retain the ones that are classified as descriptions while dicarding the rest.\n",
        "\n",
        "The model is based on the BERT model. We first import all necessary libraries and define the essential components; then we load the trained checkpoint and perform classification.\n",
        "\n",
        "Requires: python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "ty56bNnnF-tH"
      },
      "id": "ty56bNnnF-tH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the target folder in Colab\n",
        "target_folder = 'models'\n",
        "os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "# Google Drive File ID\n",
        "file_id = '1KL-mss77skgV7OtmWeBOzegJgQUcFY2s'\n",
        "\n",
        "# Destination path\n",
        "destination_path = os.path.join(target_folder, 'saved_weights_BERT_description_classifier.pt')\n",
        "\n",
        "# Download file\n",
        "!gdown --id $file_id -O $destination_path"
      ],
      "metadata": {
        "id": "K2B2A2g4Ypsc"
      },
      "id": "K2B2A2g4Ypsc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba62eae3-9aaf-4e54-9fb3-9aa58a1dfb5c",
      "metadata": {
        "id": "ba62eae3-9aaf-4e54-9fb3-9aa58a1dfb5c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import cuda\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import warnings\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Load some utilities\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "bert = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "\n",
        "# define the base bert model that we finetune for description classification\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        # Distil Bert model\n",
        "        self.bert = bert\n",
        "        ## Additional layers\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # Relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "        # Dense layer 1\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        # Dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        # Softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, **kwargs):\n",
        "\n",
        "        #pass the inputs to the model BERT\n",
        "        cls_hs = self.bert(**kwargs)\n",
        "        hidden_state = cls_hs.last_hidden_state\n",
        "        pooler = hidden_state[:, 0]\n",
        "\n",
        "        # dense layer 1\n",
        "        x = self.fc1(pooler)\n",
        "        # ReLU activation\n",
        "        x = self.relu(x)\n",
        "        # Drop out\n",
        "        x = self.dropout(x)\n",
        "        # dense layer 2\n",
        "        x = self.fc2(x)\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define an instance of the BERT model and load the weights\n",
        "model = BERT(bert).to(device)\n",
        "\n",
        "modelname = \"saved_weights_BERT_description_classifier.pt\"\n",
        "location = \"models/\"\n",
        "\n",
        "model_save_name = modelname\n",
        "path = location + model_save_name\n",
        "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb12a22f-61f6-4b6c-a818-a2990a337982",
      "metadata": {
        "id": "fb12a22f-61f6-4b6c-a818-a2990a337982"
      },
      "source": [
        "### Classification Function and Processing Functions\n",
        "---\n",
        "\n",
        "Having defined the description classifier, we now construct the appropriate functions for classifying the cleaned texts to descriptions. Instead of directly classifying the whole paragraph, it is first split into sentences using the *SpaCy* package. We store both individual sentences that were classified as descriptions, as well as new paragraphs containing only the descriptive sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5581601-5f18-4f9c-a102-2f1bb082547c",
      "metadata": {
        "id": "c5581601-5f18-4f9c-a102-2f1bb082547c"
      },
      "outputs": [],
      "source": [
        "def classify_text(span, model, truncation=True, device = 'cpu'):\n",
        "\n",
        "    \"\"\"\n",
        "    Uses a trained bert classifier to see if a span\n",
        "    belongs to a species description or not.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=truncation).to(device)\n",
        "        # Predict class\n",
        "        outputs = model(**inputs)\n",
        "        # Get prediction values\n",
        "        exps = outputs.softmax(1)\n",
        "        # Get class\n",
        "        span_class = (exps[:,1]>0.6).item()\n",
        "\n",
        "        return span_class\n",
        "\n",
        "\n",
        "def paragraph_to_descriptions(paragraph_dict, device = 'cpu'):\n",
        "    \"\"\"Converts a dictionary of paragraphs to descriptions for each species.\n",
        "\n",
        "    Args:\n",
        "        paragraph_dict (dict): A dictionary where keys are species and values\n",
        "            are lists of paragraphs.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, List[str]], Dict[str, List[str]]]: A tuple of two\n",
        "        dictionaries. The first dictionary contains the descriptions for each\n",
        "        species where each description is a concatenation of several sentences.\n",
        "        The second dictionary contains the sentences for each species that\n",
        "        passed a classification check.\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize dictionaries\n",
        "    description_paragraph_dict = collections.defaultdict(list)\n",
        "    description_sentence_dict = collections.defaultdict(list)\n",
        "\n",
        "    # Process each species\n",
        "    for (species, paragraphs) in tqdm(paragraph_dict.items(), desc=\"Species\", leave=True, position=0):\n",
        "\n",
        "        # Process each paragraph\n",
        "        for paragraph in tqdm(paragraphs, desc=f\"Paragraphs Species: {species}\", leave=False, position=0):\n",
        "\n",
        "            # Ignore very long paragraphs\n",
        "            if len(paragraph) > 80000:\n",
        "                continue\n",
        "\n",
        "            # Parse paragraph with spaCy\n",
        "            doc = nlp(paragraph)\n",
        "\n",
        "            # Store sentences that pass classification check\n",
        "            new_paragraph = []\n",
        "            for sent in doc.sents:\n",
        "                if classify_text(sent.text, model=model, device = device):\n",
        "                    description_sentence_dict[species].append(sent.text)\n",
        "                    new_paragraph.append(sent.text)\n",
        "\n",
        "            # Store paragraph if it has valid sentences\n",
        "            if new_paragraph:\n",
        "                description_paragraph_dict[species].append(' '.join(new_paragraph))\n",
        "\n",
        "    return description_paragraph_dict, description_sentence_dict\n",
        "\n",
        "# classify sentences and save\n",
        "description_sentence_dict, description_paragraph = paragraph_to_descriptions(species_text_cleaned, device = device)\n",
        "\n",
        "description_sentence_path = f'{path_to_save}description_sentences.pkl'\n",
        "description_paragraphs_path = f'{path_to_save}description_paragraphs.pkl'\n",
        "\n",
        "with open(description_sentence_path, 'wb') as f:\n",
        "    pickle.dump(description_sentence_dict, f)\n",
        "\n",
        "with open(description_paragraphs_path, 'wb') as f:\n",
        "    pickle.dump(description_paragraph, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d57836e",
      "metadata": {
        "id": "4d57836e"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(depth=4, width=500)\n",
        "pp.pprint(description_sentence_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81990fe9-eca6-4f7c-9b6e-f5414f92f795",
      "metadata": {
        "id": "81990fe9-eca6-4f7c-9b6e-f5414f92f795"
      },
      "source": [
        "### LLM Prompting\n",
        "---\n",
        "\n",
        "We now have the processed and classified text for each species. With this data at hand, we can prompt an LLM to explore if we can find information about species traits. First, we need to define the LLM client, e.g. MistralClient, load/set the traits that we want to epxlore and then use the sentences from the previous snippet to construct a prompt that we feed to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mistralai"
      ],
      "metadata": {
        "id": "Qinnc5LVIIUI"
      },
      "id": "Qinnc5LVIIUI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79f07aa-f9b5-455d-9e18-3d026db32601",
      "metadata": {
        "id": "a79f07aa-f9b5-455d-9e18-3d026db32601"
      },
      "outputs": [],
      "source": [
        "# Set Up the mistral/LLM API credentials\n",
        "from mistralai import Mistral, UserMessage\n",
        "\n",
        "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "model = \"mistral-medium-latest\"\n",
        "\n",
        "client = Mistral(api_key=mistral_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q93LhzaRDx_N"
      },
      "outputs": [],
      "source": [
        "data_folder = 'Extracted_data/'\n",
        "\n",
        "# define the name of the dataset\n",
        "results_folder = f'{data_folder}Prompt_Results/Mistral/{dataset_name}_dataset'\n",
        "os.makedirs(results_folder, exist_ok = True)\n",
        "\n",
        "# also save the traits dictionary in a text file\n",
        "with open(f'{results_folder}/traits.txt', 'w') as f:\n",
        "    for tname in traits_dict:\n",
        "        f.write('{}: {}\\n'.format(tname, traits_dict[tname]))"
      ],
      "id": "q93LhzaRDx_N"
    },
    {
      "cell_type": "markdown",
      "id": "70f74b9e-847a-4825-9eb5-ccaae4e77189",
      "metadata": {
        "id": "70f74b9e-847a-4825-9eb5-ccaae4e77189"
      },
      "source": [
        "### LLM Prompting: Main Loop\n",
        "---\n",
        "\n",
        "Having loaded the traits, we proceed with the core loop that is responsible for the prompting of the LLM. For each species and its sentences, we construct an appropriate prompt (see the *text* variable below) that is sent to the LLM via the client chat function. We record the response of the LLM, making sure that it is in an appropriate form (JSON) before proceeding; otherwise we re=prompt the LLM. We save the results in human-readable form, i.e., plain text format. Then, we can proceed with the aggregation of the information for all the species."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21A4jC64Dx_N"
      },
      "outputs": [],
      "source": [
        "# traits are in traits\n",
        "traits_names = list(traits_dict.keys())\n",
        "traits_names_cap = [ tr.capitalize() for tr in traits_dict.keys() ]\n",
        "\n",
        "# parse all the species\n",
        "for idx, species in enumerate(description_sentence_dict):\n",
        "\n",
        "\n",
        "    print('Cur Species Num: {}/{} Name: {}'.format(idx+1, len(description_sentence_dict), species))\n",
        "\n",
        "\n",
        "    # create the folder for the species. replace blanks with underscores to avoid potential issues\n",
        "    species_folder = results_folder + '/{}'.format(species.replace(' ', '_'))\n",
        "    os.makedirs(species_folder, exist_ok = True)\n",
        "\n",
        "\n",
        "    # this is the list with all the sentences, we are gonna iterate and combine.\n",
        "    sentences_ = description_sentence_dict[species]\n",
        "\n",
        "    #print(sentences_)\n",
        "    # save cleaned sentences in a txt format\n",
        "    with open(f'{species_folder}/sentences_cleaned.txt', 'w') as f:\n",
        "        for sent in sentences_:\n",
        "            f.write(sent+'\\n')\n",
        "\n",
        "    # all the reponses and contents only for the species\n",
        "    responses_full = []\n",
        "    contents = []\n",
        "    llm_dict_traits = {}\n",
        "\n",
        "    cur_paragraph = '\\n'.join(sentences_)\n",
        "\n",
        "    pos_traits = '{'\n",
        "    for j in range(0, len(traits_names)):\n",
        "        pos_traits += '\\\"{}\\\": {}, '.format(traits_names[j].capitalize(), traits_dict[traits_names[j]]                                    )\n",
        "        # until the third to last element to remove comma and space...\n",
        "    pos_traits = pos_traits[:-2] + '}'\n",
        "\n",
        "\n",
        "    text = 'We are interested in obtaining botanical trait information about the species {}.\\n\\n'.format(species)\n",
        "    text += 'We will provide an input text with botanical descriptions,'\\\n",
        "            'followed by a dictionary where each key \\'name\\' represents a trait name, '\\\n",
        "            'referring to specific organ or other element of the plant, and is associated to a list '\\\n",
        "            'with all possible trait values for that trait, [\\'value_1\\', \\'value_2\\', ..., \\'value_n\\'].\\n\\n'\n",
        "\n",
        "    text += 'Input text:\\n'\n",
        "    text += cur_paragraph +'\\n\\n'\n",
        "\n",
        "    text += 'Initial dictionary of traits with all possible values:\\n'\n",
        "    text += pos_traits +'\\n\\n'\n",
        "\n",
        "    text += 'Turn each string s in the list of values in the dictionary into a sublist (s,b), where b is a binary number,'\\\n",
        "             'either 0 or 1, indicating whether there is strong evidence for value s in the input text. '\n",
        "    text+= 'Double check that \\'value_i\\' is reported referring to trait \\'name\\' in the text, '\\\n",
        "            'and not to a different trait. Set \\'b\\' to \\'0\\' if you are not sure about '\\\n",
        "            'the association. Do not modify the initial trait names and trait values, and do not add other traits than those provided in the dictionary. '\\\n",
        "            'Return the dictionary of traits and sublists of (value, evidence) containing all possible names and (value, evidence) tuples.\\n\\n'\n",
        "    text += 'Output first a dictionary in JSON format, followed by a very short textual explanation for each positive response.\\n\\n'\n",
        "\n",
        "\n",
        "    cur_path = '{}/results/'.format(species_folder)\n",
        "\n",
        "    os.makedirs(cur_path, exist_ok = True)\n",
        "\n",
        "    messages = [{'role': \"user\",\n",
        "                 'content': text}]\n",
        "\n",
        "    retries = 3\n",
        "    while retries>0:\n",
        "        try:\n",
        "\n",
        "            chat_response = client.chat.complete(\n",
        "                model=model,\n",
        "                #response_format={\"type\": \"json_object\"},\n",
        "                messages=messages,\n",
        "            )\n",
        "            chat_response = chat_response.choices[0].message.content\n",
        "            content = re.search(r'{.*}', chat_response, re.DOTALL).group()\n",
        "            content_as_json = json.loads(content)\n",
        "\n",
        "            retries = 0.\n",
        "            break\n",
        "        except (Exception, JSONDecodeError) as e:\n",
        "            if e:\n",
        "                print('Some Kind of Error, {}'.format(e))\n",
        "                retries -= 1\n",
        "                time.sleep(5)\n",
        "\n",
        "\n",
        "    with open('{}/mistral_response_full.txt'.format(cur_path), 'w') as f:\n",
        "        f.write(str(chat_response))\n",
        "    with open('{}/mistral_response_content_only.txt'.format(cur_path), 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    with open('{}/mistral_sent_info_and_content.txt'.format(cur_path), 'w') as f:\n",
        "        f.write('{}\\n\\n{}'.format(text, content))\n",
        "\n",
        "    responses_full.append(str(chat_response))\n",
        "    contents.append(content)\n",
        "\n",
        "\n",
        "    with open('{}/responses_all.txt'.format(species_folder), 'w') as f:\n",
        "        for resp in responses_full:\n",
        "            f.write(resp + '\\n\\n')\n",
        "    with open('{}/contents_all.txt'.format(species_folder), 'w') as f:\n",
        "        for cont in contents:\n",
        "            f.write(cont + '\\n\\n')\n",
        "\n"
      ],
      "id": "21A4jC64Dx_N"
    },
    {
      "cell_type": "markdown",
      "id": "62ad3469-b0d0-4bca-8933-a44c3e12045b",
      "metadata": {
        "id": "62ad3469-b0d0-4bca-8933-a44c3e12045b"
      },
      "source": [
        "### Post Processing\n",
        "---\n",
        "\n",
        "All the responses of the LLM for each species are saved in their corresponding folders. All that is left is to aggregate the information from the individual species and output the results. We use the functions defined in the *aggregate_traits.py* file, found in the notebook's folder, and specifically the *post_processing* function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "############## HELPER FUNCTIONS ########################################\n",
        "########################################################################\n",
        "def replace_wrong_values(spec_traits, trait, wrong_trait_value, correct_trait_value):\n",
        "    \"\"\"\n",
        "\n",
        "    :param spec_traits:\n",
        "    :param trait:\n",
        "    :param wrong_trait_value:\n",
        "    :param correct_trait_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if trait in spec_traits:\n",
        "        if wrong_trait_value in spec_traits[trait]:\n",
        "            if correct_trait_value in spec_traits[trait]:\n",
        "                if spec_traits[trait][wrong_trait_value] == '1' or spec_traits[trait][correct_trait_value] == '1':\n",
        "                    spec_traits[trait][correct_trait_value] = '1'\n",
        "            else:\n",
        "                spec_traits[trait][correct_trait_value] = spec_traits[trait][wrong_trait_value]\n",
        "            del spec_traits[trait][wrong_trait_value]\n",
        "\n",
        "    return spec_traits\n",
        "\n",
        "\n",
        "def replace_wrong_key(spec_traits, wrong_trait, correct_trait):\n",
        "    \"\"\"\n",
        "    There are cases where the LLM returns the wrong trait name, e.g. color instead of colour.\n",
        "    Use this function to fix these kind of inconsistencies. This function also checks if the correct trait\n",
        "    is alreaady in the dictionary and does some merging of the values.\n",
        "\n",
        "    :param spec_traits: the dictionary of traits for a species.\n",
        "    :param wrong_trait: the name of the wrong trait in the dictionary.\n",
        "    :param correct_trait: the correct trait that should be in the dictionary.\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if wrong_trait in spec_traits:\n",
        "        if correct_trait not in spec_traits:\n",
        "            spec_traits[correct_trait] = spec_traits[wrong_trait]\n",
        "            del spec_traits[wrong_trait]\n",
        "        else:\n",
        "            for val in spec_traits[wrong_trait]:\n",
        "                if val not in traits[correct_trait.title()]:\n",
        "                    continue\n",
        "                if val not in spec_traits[correct_trait]:\n",
        "                    spec_traits[correct_trait][val] = spec_traits[wrong_trait][val]\n",
        "                else:\n",
        "                    if spec_traits[wrong_trait][val] == '1' or spec_traits[correct_trait][val] == '1':\n",
        "                        spec_traits[correct_trait][val] = '1'\n",
        "\n",
        "            del spec_traits[wrong_trait]\n",
        "\n",
        "    return spec_traits\n",
        "\n",
        "\n",
        "def del_wrong_value(spec_traits, trait, wrong_trait_value):\n",
        "    \"\"\"\n",
        "    Delete potential wrong value in the obtained dictionary of traits.\n",
        "    LLMs tend to introduce some values or misspell some. In the latter, maybe check if you should\n",
        "    replace instead of delete.\n",
        "\n",
        "    :param spec_traits: the dictionary of traits for the species\n",
        "    :param trait: the current trait in which the given wrong value is present\n",
        "    :param wrong_trait_value: the wrong trait value to delete from the dictionary\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if trait not in spec_traits:\n",
        "        return spec_traits\n",
        "\n",
        "    if wrong_trait_value in spec_traits[trait]:\n",
        "        del spec_traits[trait][wrong_trait_value]\n",
        "\n",
        "    return spec_traits\n",
        "\n",
        "#####################################################\n",
        "################## MAIN LOGIC #######################\n",
        "#####################################################\n",
        "def post_processing(traits_gt, species_gt, responses_paths):\n",
        "\n",
        "    # Make a header for the csv file as was the original\n",
        "    header_traits = [' ']\n",
        "    header_values = [' ']\n",
        "\n",
        "    # for all the traits keys\n",
        "    for key in traits_gt:\n",
        "        # for all the the trait values\n",
        "        for value in traits_gt[key]:\n",
        "            header_values.append(value)\n",
        "            header_traits.append(key)\n",
        "\n",
        "    traits_gt = {k.lower(): [val.lower() for val in v] for k, v in traits_gt.items()}\n",
        "\n",
        "    # this walk can help us check if we have data for the species from the ground truth\n",
        "    dir_list = next(os.walk(responses_paths))[1]\n",
        "\n",
        "    # we are going to build a file similar to the original for easy comparison\n",
        "    rows = []\n",
        "    species_wide_llm_traits = {}\n",
        "    with open(f'summary.csv', 'w', encoding='UTF-8', newline='') as csvf:\n",
        "\n",
        "        writer = csv.writer(csvf)\n",
        "\n",
        "        # write the header stuff first\n",
        "        writer.writerow(header_traits)\n",
        "        writer.writerow(header_values)\n",
        "        writer.writerow(['Species'])\n",
        "\n",
        "        # each folder is a species\n",
        "        i = 0\n",
        "\n",
        "        # you can sort first if you want\n",
        "        # species_keys = sorted(list(dict_gt.keys()))\n",
        "        for species in species_gt:\n",
        "            cur_spec = species\n",
        "            species = species.replace(' ', '_')\n",
        "\n",
        "            # if the species is not in the folder list, we didn't have data for that\n",
        "            # unless something went wrong with the script\n",
        "            if species not in dir_list:\n",
        "                print('Spec: {} not in dir list.'.format(species))\n",
        "                writer.writerow([cur_spec] + ['-'] * (len(header_values) - 1))\n",
        "                continue\n",
        "\n",
        "            # now read the contents_all file that was saved from the response of the LLM\n",
        "            # when we were trying MISTRAL, the json functionality was not working, so we need to do some\n",
        "            # parsing. This needs to be changed depending on what you ask the LLM output to be.\n",
        "            with open(f'{responses_paths}/{species}/contents_all.txt', 'r') as f:\n",
        "                data = f.read().replace('(', '[').replace(')', ']')\n",
        "                content_as_json = json.loads(data)\n",
        "\n",
        "            # the code after the next code line performs some checks assessing if the traits and the values are correct\n",
        "            # there are two options: 1) delete the wrong traits/values, ignore them, or do replacements.\n",
        "            # thus, you can use the functions in the first lines to do that and recheck.\n",
        "            # content_as_json = replace_wrong_key(content_as_json, 'Sepals calyx number', 'Sepals calyx numer')\n",
        "\n",
        "            # now we are parsing the response to get the traits and their values\n",
        "            # while checking at the same time that everything is valid.\n",
        "            # spec traits will contain all the correct traits/values for the current species.\n",
        "            spec_traits = {}\n",
        "            for key in content_as_json:\n",
        "                lower_key = key.lower()\n",
        "\n",
        "                # this checks if the trait name is in the ground truth\n",
        "                # here we ignore the wrong values and don't add it to the spec_traits dict\n",
        "                # you can raise the nameerror to see what's wrong and add some lines before the loop to correct it.\n",
        "                if lower_key not in traits_gt:\n",
        "                    #print(species_wide_gt_traits[cur_spec].keys())\n",
        "                    print('Trait: \\\"{}\\\" not in the ground truth. Ignoring..'.format(lower_key))\n",
        "                    continue\n",
        "\n",
        "                # if trait is correct put it in the dict.\n",
        "                if lower_key not in spec_traits:\n",
        "                    spec_traits[lower_key] = {}\n",
        "\n",
        "                # now we check how the values are formatted. You may get values as strings, tuples, lists.\n",
        "                # we want to build a dict after that.\n",
        "                if isinstance(content_as_json[key], list):\n",
        "\n",
        "                    for trait_values in content_as_json[key]:\n",
        "\n",
        "                        # first check the format\n",
        "                        if isinstance(trait_values, str):\n",
        "                            trait_value, presence = eval(trait_values)\n",
        "                        elif isinstance(trait_values, list) or isinstance(trait_values, tuple):\n",
        "                            trait_value, presence = trait_values\n",
        "                        else:\n",
        "                            raise ValueError('Wrong format: {}'.format(trait_values))\n",
        "\n",
        "                        # here check if trait value is not in the gt values\n",
        "                        # in our case, we ignore it, but you can raise the nameerror and fix it before the trait loop.\n",
        "                        if trait_value.lower() not in traits_gt[lower_key]:\n",
        "                            #print(species_wide_gt_traits[cur_spec][key.lower()])\n",
        "                            print('Trait value: \\\"{}\\\" for key: \\\"{}\\\" is not in the gt'.format(trait_value, key))\n",
        "                            continue\n",
        "\n",
        "                        # if the value is not in the dict, add it\n",
        "                        # if for some reason it is, check the value.\n",
        "                        if trait_value not in spec_traits[lower_key]:\n",
        "                            spec_traits[lower_key][trait_value.lower()] = presence\n",
        "                        elif spec_traits[lower_key][trait_value.lower()] == 1:\n",
        "                            continue\n",
        "                        else:\n",
        "                            spec_traits[lower_key][trait_value.lower()] = presence\n",
        "                else:\n",
        "                    raise ValueError('Wrong format..')\n",
        "\n",
        "            # This is the final check.\n",
        "            # Take all the traits from the ground truth and make sure it's there.\n",
        "            # do the same for the values.\n",
        "            # if some value is missing, add it with a zero value.\n",
        "            for key in traits_gt:\n",
        "\n",
        "                if key not in spec_traits:\n",
        "                    spec_traits[key] = {}\n",
        "                    print('Trait: \\\"{}\\\" not found in the constructed species traits. Adding entry..'.format(key))\n",
        "\n",
        "                for val in traits_gt[key]:\n",
        "                    if val not in spec_traits[key]:\n",
        "                        print('Value: \\\"{}\\\" of trait: \\\"{}\\\" not found in '\n",
        "                                        'the constructed species traits.'.format(val, key))\n",
        "                        # didn't find it through the LLM, add it with a zero value\n",
        "                        spec_traits[key][val.lower()] = 0\n",
        "\n",
        "            # if you get no output from the above, we can build the final list of values\n",
        "            # build the list of elements to write to csv. First is species names.\n",
        "            cur_values = [cur_spec]\n",
        "            for key in spec_traits:\n",
        "                for val in spec_traits[key]:\n",
        "                    cur_values.append(spec_traits[key][val])\n",
        "\n",
        "            # write to csv\n",
        "            writer.writerow(cur_values)\n",
        "            rows.append(cur_values)\n",
        "\n",
        "            # add the species dict to the general llm trait dictionary\n",
        "            species_wide_llm_traits[species.replace('_', ' ')] = spec_traits"
      ],
      "metadata": {
        "id": "UBACgIL3TdaM"
      },
      "id": "UBACgIL3TdaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put together and display the results\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Fetch the trait disctionaries and produce a summary of all species in \"summary.csv\"\n",
        "post_processing(traits_dict, species_to_query,results_folder)\n",
        "\n",
        "# Path to the file\n",
        "file_path = \"summary.csv\"\n",
        "\n",
        "# Read the CSV file\n",
        "rows = []\n",
        "with open(file_path, 'r') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    rows = list(csv_reader)\n",
        "\n",
        "# Generate HTML table\n",
        "html_table = '<table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">'\n",
        "for row in rows:\n",
        "    html_table += '<tr>'\n",
        "    for cell in row:\n",
        "        html_table += f'<td style=\"padding: 5px;\">{cell}</td>'\n",
        "    html_table += '</tr>'\n",
        "html_table += '</table>'\n",
        "\n",
        "# Wrap the table in a scrollable container\n",
        "scrollable_html = f'''\n",
        "<div style=\"max-height: 400px; overflow-y: scroll; border: 1px solid #ccc;\">\n",
        "  {html_table}\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# Display the scrollable table\n",
        "display(HTML(scrollable_html))"
      ],
      "metadata": {
        "id": "dnKXIXTvXtE6"
      },
      "id": "dnKXIXTvXtE6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}