{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import warnings\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import spacy\n",
    "import json\n",
    "import glob\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')  \n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        # Distil Bert model\n",
    "        self.bert = bert\n",
    "        ## Additional layers\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # Dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # Dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        # Softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, **kwargs):\n",
    "\n",
    "        #pass the inputs to the model BERT  \n",
    "        cls_hs = self.bert(**kwargs)\n",
    "        hidden_state = cls_hs.last_hidden_state\n",
    "        pooler = hidden_state[:, 0]\n",
    "\n",
    "        # dense layer 1        \n",
    "        x = self.fc1(pooler)\n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Drop out\n",
    "        x = self.dropout(x)\n",
    "        # dense layer 2\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT(bert)\n",
    "model = model.to(device)\n",
    "\n",
    "modelname = \"saved_weights_BERT_description_classifier.pt\"\n",
    "location = \"../../../models/saved_weights/\"\n",
    "\n",
    "model_save_name = modelname\n",
    "path = location + model_save_name\n",
    "model.load_state_dict(torch.load(path, \n",
    "                                    map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(span, model, truncation=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or otherwise.\n",
    "    \"\"\"\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=truncation)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "        # Get prediction values\n",
    "        exps = torch.exp(outputs)\n",
    "        # Get class\n",
    "        span_class = exps.argmax(1).item()\n",
    "\n",
    "        return span_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_text = \"TextSnippetsCleaned/\"\n",
    "\n",
    "# caribbean_text_dict = pickle.load(open(F\"{folder_text}paragraphs_caribbean_cleaned.pkl\", 'rb'))\n",
    "# palms_text_dict = pickle.load(open(F\"{folder_text}paragraphs_palms_cleaned.pkl\", 'rb'))\n",
    "west_text_dict = pickle.load(open(F\"{folder_text}paragraphs_west_cleaned.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_to_descriptions(paragraph_dict):\n",
    "    \"\"\"Converts a dictionary of paragraphs to descriptions for each species.\n",
    "\n",
    "    Args:\n",
    "        paragraph_dict (dict): A dictionary where keys are species and values\n",
    "            are lists of paragraphs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, List[str]], Dict[str, List[str]]]: A tuple of two \n",
    "        dictionaries. The first dictionary contains the descriptions for each \n",
    "        species where each description is a concatenation of several sentences.\n",
    "        The second dictionary contains the sentences for each species that \n",
    "        passed a classification check.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries\n",
    "    description_paragraph_dict = collections.defaultdict(list)\n",
    "    description_sentence_dict = collections.defaultdict(list)\n",
    "\n",
    "    # Process each species\n",
    "    for (species, paragraphs) in tqdm(paragraph_dict.items(), desc=\"Species\", leave=True, position=0):\n",
    "\n",
    "        # Process each paragraph\n",
    "        for paragraph in tqdm(paragraphs, desc=\"Paragraph\", leave=False, position=0):\n",
    "\n",
    "            # Ignore very long paragraphs\n",
    "            if len(paragraph) > 800000:\n",
    "                continue\n",
    "            \n",
    "            # Parse paragraph with spaCy\n",
    "            doc = nlp(paragraph)\n",
    "\n",
    "            # Store sentences that pass classification check\n",
    "            new_paragraph = []\n",
    "            for sent in doc.sents:\n",
    "                if classify_text(sent.text, model=model):\n",
    "                    description_sentence_dict[species].append(sent.text)\n",
    "                    new_paragraph.append(sent.text)\n",
    "\n",
    "            # Store paragraph if it has valid sentences\n",
    "            if new_paragraph:\n",
    "                description_paragraph_dict[species].append(' '.join(new_paragraph))\n",
    "\n",
    "    return description_paragraph_dict, description_sentence_dict\n",
    "\n",
    "\n",
    "def species_paragraphs_to_json(text_dict):\n",
    "\n",
    "    \"\"\"Converts species descriptions from dictionary format to JSON format, and saves\n",
    "    the resulting JSON files to specified folders.\n",
    "\n",
    "    Args:\n",
    "        text_dict (dict): A dictionary of species descriptions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    folder_paragraphs = \"DescriptionSnippets/Paragraphs/\"\n",
    "    folder_sentences = \"DescriptionSnippets/Sentences/\"\n",
    "\n",
    "    for idx, (species, paragraphs) in enumerate(text_dict.items()): \n",
    "\n",
    "        # Folder Naming\n",
    "        species_name = species.replace(' ', '_')\n",
    "        file_name_sent = F\"west_{species_name}_descriptions_sentences\"\n",
    "        file_name_para = F\"west_{species_name}_descriptions_paragraphs\"\n",
    "\n",
    "        # Files Already done\n",
    "        jsons_done = glob.glob(\"DescriptionSnippets/Paragraphs/*\")\n",
    "        final_name = F\"{folder_paragraphs}{file_name_para}.json\"\n",
    "        final_name = F\"{folder_paragraphs}{file_name_para}.json\"\n",
    "        if final_name in jsons_done:\n",
    "            continue\n",
    "\n",
    "        # Init dict\n",
    "        description_paragraph_dict = collections.defaultdict(list)\n",
    "        description_sentence_dict = collections.defaultdict(list)\n",
    "\n",
    "        for paragraph in (pbar := tqdm(paragraphs, leave=False, position=0)):\n",
    "            pbar.set_description(f\"{idx} {species}\")\n",
    "\n",
    "            # Not able to tokenize\n",
    "            if len(paragraph) > 50000: # Was 800.000\n",
    "                continue\n",
    "\n",
    "            doc = nlp(paragraph)\n",
    "            new_paragraph = []\n",
    "\n",
    "            for sent in doc.sents:\n",
    "\n",
    "                if classify_text(sent.text, model=model):\n",
    "\n",
    "                    description_sentence_dict[species].append(sent.text)\n",
    "                    new_paragraph.append(sent.text)\n",
    "\n",
    "            if new_paragraph:\n",
    "                description_paragraph_dict[species].append(' '.join(new_paragraph))\n",
    "\n",
    "        with open(F\"{folder_sentences}{file_name_sent}.json\", 'w') as fp:\n",
    "            json.dump(description_sentence_dict, fp)\n",
    "\n",
    "        with open(F\"{folder_paragraphs}{file_name_para}.json\", 'w') as fp:\n",
    "            json.dump(description_paragraph_dict, fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caribbean_text_dict = {k: caribbean_text_dict[k][0:20] for k in list(caribbean_text_dict)[:4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47 Blighia sapida:  74%|███████▍  | 358/481 [01:06<00:18,  6.68it/s]                       Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "species_paragraphs_to_json(west_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_text = \"../../../data/OpenAI/DescriptionSnippets/\"\n",
    "\n",
    "# with open(F\"{folder_text}descriptions_paragraphs_caribbean.pkl\", 'wb') as f:\n",
    "#     pickle.dump(caribbean_description_paragraph_dict, f)\n",
    "# with open(F\"{folder_text}descriptions_sentences_caribbean.pkl\", 'wb') as f:\n",
    "#     pickle.dump(caribbean_description_sentence_dict, f)\n",
    "\n",
    "# with open(F\"{folder_text}descriptions_paragraphs_palms.pkl\", 'wb') as f:\n",
    "#     pickle.dump(palms_description_paragraph_dict, f)\n",
    "# with open(F\"{folder_text}descriptions_sentences_palms.pkl\", 'wb') as f:\n",
    "#     pickle.dump(palms_description_sentence_dict, f)\n",
    "\n",
    "# with open(F\"{folder_text}descriptions_paragraphs_west.pkl\", 'wb') as f:\n",
    "#     pickle.dump(west_description_paragraph_dict, f)\n",
    "# with open(F\"{folder_text}descriptions_sentences_west.pkl\", 'wb') as f:\n",
    "#     pickle.dump(west_description_sentence_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "020a6b5c0ed803a704c00010560cf50a059d086f11791eecdae2eedc39704b9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
