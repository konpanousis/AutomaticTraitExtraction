{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337169f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42388b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up your LLM here, make sure to gave the api key in a dot env \n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-medium-latest\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_words_with_capital(string):\n",
    "    # remove non-alphanumeric characters\n",
    "    string = re.sub(r'[^\\w\\s/]', '', string)\n",
    "    # split the string on the slash (\"/\")\n",
    "    parts = string.split('/')\n",
    "    # combine words with capitalization for each part\n",
    "    parts = [''.join(word.capitalize() for word in part.split()) for part in parts]\n",
    "    # join the parts with an empty string\n",
    "    return ''.join(parts)\n",
    "\n",
    "def create_clean_paragraphs(input_dict):\n",
    "\n",
    "    # Create a new dictionary to store the cleaned-up values\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Loop through the keys and values of the input dictionary\n",
    "    for key, value in input_dict.items():\n",
    "        # Convert the list of values to a set to remove duplicates\n",
    "        unique_values = set(value)\n",
    "        \n",
    "        # Join the sentences together into a single string\n",
    "        combined_string = ' '.join(unique_values)\n",
    "        \n",
    "        # Add the cleaned-up string to the output dictionary\n",
    "        output_dict[key] = combined_string\n",
    "    \n",
    "    # Return the cleaned-up dictionary\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "# this is new \n",
    "def clean_sentence(sentence):\n",
    "    sent = sentence.replace('Â','').replace('â', '-').replace('·','.').replace('Ã','x').replace(u'\\xa0', u' ')\n",
    "    sent = sent.replace('â', '').replace('â', '-').replace('x©', 'e').strip()\n",
    "    \n",
    "    return sent\n",
    "\n",
    "def create_clean_and_unique_sentences(input_dict):\n",
    "    output_dict = {}\n",
    "    \n",
    "    for key, value in input_dict.items():\n",
    "        unique_values = [clean_sentence(sent) for sent in set(value) if len(sent.split(' '))>=2]\n",
    "        \n",
    "        output_dict[key] = unique_values\n",
    "        \n",
    "    return output_dict\n",
    "        \n",
    "    \n",
    "def create_even_cleaner_paragraphs(input_dict):\n",
    "    # Create a new dictionary to store the cleaned-up values\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Loop through the keys and values of the input dictionary\n",
    "    for key, value in input_dict.items():\n",
    "        # Convert the list of values to a set to remove duplicates\n",
    "        unique_values = set(value)\n",
    "        \n",
    "        # Join the sentences together into a single string\n",
    "        combined_string = ' '.join(unique_values)\n",
    "        \n",
    "        # Add the cleaned-up string to the output dictionary (these are new)\n",
    "        output_dict[key] = clean_sentence(combined_string)\n",
    "    \n",
    "    # Return the cleaned-up dictionary\n",
    "    return output_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some data paths \n",
    "data_folder = 'Data/'\n",
    "dataframes_folder = data_folder + 'DataFrames/'\n",
    "traits_folder = data_folder + 'Traits/'\n",
    "sentences_folder = data_folder + 'Sentences/'\n",
    "\n",
    "# define the name of the dataset \n",
    "dataset = 'West' # Caribbean, Palm or West\n",
    "results_folder = 'Results/Mistral/{}'.format(dataset)\n",
    "os.makedirs(results_folder, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the traits. we are going to use this to query the LLM\n",
    "with open(F\"{traits_folder}{dataset}.json\", 'r') as f:\n",
    "  traits_dict = json.load(f)\n",
    "\n",
    "print(traits_dict.keys())\n",
    "print(len(traits_dict))\n",
    "\n",
    "if dataset == 'Palm':\n",
    "    del traits_dict['Measurement']\n",
    "    print(traits_dict.keys())\n",
    "    print(len(traits_dict))\n",
    "\n",
    "# also save the traits in a more user friendly form, i.e., text \n",
    "with open('{}/traits.txt'.format(results_folder), 'w') as f:\n",
    "    for tname in traits_dict:\n",
    "        f.write('{}: {}\\n'.format(tname, traits_dict[tname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read the sentences/paragraphs. These are the relevant texts from which we want to extract traits. \n",
    "sentences_file = sentences_folder + 'Sents_{}.pkl'.format(dataset)\n",
    "with open(sentences_file, 'rb') as f:\n",
    "    sentences = pickle.load(f, encoding='utf8')\n",
    "sentences_cleaned = create_clean_and_unique_sentences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_per_query = 4\n",
    "num_traits_per_query = 4\n",
    "with open('{}/settings.txt'.format(results_folder), 'w') as f:\n",
    "    f.write('Num sentences per query: {}\\nNum Traits per Query: {}'.format(num_sentences_per_query, num_traits_per_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traits are in traits\n",
    "traits_names = list(traits_dict.keys())\n",
    "traits_names_cap = [ tr.capitalize() for tr in traits_dict.keys() ]\n",
    "    \n",
    "# parse all the species\n",
    "for idx, species in enumerate(sentences_cleaned):\n",
    "    \n",
    "    if species.replace(' ', '_') not in allowed_species:\n",
    "        continue\n",
    "    \n",
    "    print('Cur Species Num: {}/{} Name: {}'.format(idx, len(sentences_cleaned), species))\n",
    "\n",
    "    \n",
    "    # create the folder for the species. replace blanks with underscores to avoid potential issues\n",
    "    species_folder = results_folder + '/{}'.format(species.replace(' ', '_'))\n",
    "    os.makedirs(species_folder, exist_ok = True)\n",
    "    \n",
    "    \n",
    "    # this is the list with all the sentences, we are gonna iterate and combine.\n",
    "    sentences_ = sentences_cleaned[species]\n",
    "    \n",
    "    #print(sentences_)\n",
    "    # save cleaned sentences in a txt format\n",
    "    with open('{}/sentences_cleaned.txt'.format(species_folder), 'w') as f:\n",
    "        for sent in sentences_:\n",
    "            f.write(sent+'\\n')\n",
    "        \n",
    "    # all the reponses and contents only for the species \n",
    "    responses_full = []\n",
    "    contents = []\n",
    "    gpt_dict_traits = {}\n",
    "    \n",
    "    cur_paragraph = '\\n'.join(sentences_)\n",
    "    \n",
    "    pos_traits = '{'\n",
    "    for j in range(0, len(traits_names)):\n",
    "        pos_traits += '\\\"{}\\\": {}, '.format(traits_names[j].capitalize(), traits_dict[traits_names[j]]                                    )\n",
    "        # until the third to last element to remove comma and space...\n",
    "    pos_traits = pos_traits[:-2] + '}'\n",
    "     \n",
    "    \n",
    "    text = 'We are interested in obtaining botanical trait information about the species {}.\\n\\n'.format(species)\n",
    "    text += 'We will provide an input text with botanical descriptions,'\\\n",
    "            'followed by a dictionary where each key \\'name\\' represents a trait name, '\\\n",
    "            'referring to specific organ or other element of the plant, and is associated to a list '\\\n",
    "            'with all possible trait values for that trait, [\\'value_1\\', \\'value_2\\', ..., \\'value_n\\'].\\n\\n'\n",
    "    \n",
    "    text += 'Input text:\\n'\n",
    "    text += cur_paragraph +'\\n\\n'\n",
    "    \n",
    "    text += 'Initial dictionary of traits with all possible values:\\n'\n",
    "    text += pos_traits +'\\n\\n'\n",
    "    \n",
    "    text += 'Turn each string s in the list of values in the dictionary into a sublist (s,b), where b is a binary number,'\\\n",
    "             'either 0 or 1, indicating whether there is strong evidence for value s in the input text. '\n",
    "    text+= 'Double check that \\'value_i\\' is reported referring to trait \\'name\\' in the text, '\\\n",
    "            'and not to a different trait. Always set \\'b\\' to \\'0\\' if you are not 100% sure about '\\\n",
    "            'the association. Do not add new trait values and do not modify the initial ones.Return the dictionary of traits and sublists of (value, evidence) containing ALL POSSIBLE NAMES AND (values, evidence) tuples.\\n\\n'\n",
    "    text += 'Output only a dictionary in JSON format, no other text at all.\\n\\n'\n",
    "    \n",
    "\n",
    "    cur_path = '{}/results/'.format(species_folder)\n",
    "\n",
    "    os.makedirs(cur_path, exist_ok = True)\n",
    "  \n",
    "    messages = [ChatMessage(role=\"user\", content = text)]\n",
    "    \n",
    "    retries = 5\n",
    "    while retries>0:\n",
    "        try:\n",
    "\n",
    "            chat_response = client.chat(\n",
    "                model=model,\n",
    "                #response_format={\"type\": \"json_object\"},\n",
    "                messages=messages,\n",
    "            )\n",
    "            content = chat_response.choices[0].message.content\n",
    "            content_as_json = json.loads(content)\n",
    "            \n",
    "            retries = 0.\n",
    "            break\n",
    "        except (Exception, JSONDecodeError) as e:\n",
    "            if e:\n",
    "                print('Some Kind of Error, {}'.format(e))\n",
    "                retries -= 1\n",
    "                time.sleep(5)\n",
    "\n",
    "    \n",
    "    with open('{}/mistral_response_full.txt'.format(cur_path), 'w') as f:\n",
    "        f.write(str(chat_response))\n",
    "    with open('{}/mistral_response_content_only.txt'.format(cur_path), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "    with open('{}/mistral_sent_info_and_content.txt'.format(cur_path), 'w') as f:\n",
    "        f.write('{}\\n\\n{}'.format(text, content))\n",
    "\n",
    "    responses_full.append(str(chat_response))\n",
    "    contents.append(content)\n",
    "\n",
    "\n",
    "    with open('{}/responses_all.txt'.format(species_folder), 'w') as f:\n",
    "        for resp in responses_full:\n",
    "            f.write(resp + '\\n\\n')\n",
    "    with open('{}/contents_all.txt'.format(species_folder), 'w') as f:\n",
    "        for cont in contents:\n",
    "            f.write(cont + '\\n\\n')\n",
    "    \n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
