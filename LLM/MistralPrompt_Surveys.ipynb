{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd021c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import pickle\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# this is new\n",
    "from unidecode import unidecode\n",
    "from unicodedata import normalize\n",
    "import tiktoken\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794401b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# create your LLM client here\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-medium-latest\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a46df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_words_with_capital(string):\n",
    "    # remove non-alphanumeric characters\n",
    "    string = re.sub(r'[^\\w\\s/]', '', string)\n",
    "    # split the string on the slash (\"/\")\n",
    "    parts = string.split('/')\n",
    "    # combine words with capitalization for each part\n",
    "    parts = [''.join(word.capitalize() for word in part.split()) for part in parts]\n",
    "    # join the parts with an empty string\n",
    "    return ''.join(parts)\n",
    "\n",
    "def create_clean_paragraphs(input_dict):\n",
    "\n",
    "    # Create a new dictionary to store the cleaned-up values\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Loop through the keys and values of the input dictionary\n",
    "    for key, value in input_dict.items():\n",
    "        # Convert the list of values to a set to remove duplicates\n",
    "        unique_values = set(value)\n",
    "        \n",
    "        # Join the sentences together into a single string\n",
    "        combined_string = ' '.join(unique_values)\n",
    "        \n",
    "        # Add the cleaned-up string to the output dictionary\n",
    "        output_dict[key] = combined_string\n",
    "    \n",
    "    # Return the cleaned-up dictionary\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "# this is new \n",
    "def clean_sentence(sentence):\n",
    "    sent = sentence.replace('Â','').replace('â', '-').replace('·','.').replace('Ã','x').replace(u'\\xa0', u' ')\n",
    "    sent = sent.replace('â', '').replace('â', '-').replace('x©', 'e').strip()\n",
    "    \n",
    "    return sent\n",
    "\n",
    "def create_clean_and_unique_sentences(input_dict):\n",
    "    output_dict = {}\n",
    "    \n",
    "    for key, value in input_dict.items():\n",
    "        unique_values = [clean_sentence(sent) for sent in set(value) if len(sent.split(' '))>=2]\n",
    "        \n",
    "        output_dict[key] = unique_values\n",
    "        \n",
    "    return output_dict\n",
    "        \n",
    "    \n",
    "def create_even_cleaner_paragraphs(input_dict):\n",
    "    # Create a new dictionary to store the cleaned-up values\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Loop through the keys and values of the input dictionary\n",
    "    for key, value in input_dict.items():\n",
    "        # Convert the list of values to a set to remove duplicates\n",
    "        unique_values = set(value)\n",
    "        \n",
    "        # Join the sentences together into a single string\n",
    "        combined_string = ' '.join(unique_values)\n",
    "        \n",
    "        # Add the cleaned-up string to the output dictionary (these are new)\n",
    "        output_dict[key] = clean_sentence(combined_string)\n",
    "    \n",
    "    # Return the cleaned-up dictionary\n",
    "    return output_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some data paths \n",
    "data_folder = 'Data/'\n",
    "dataframes_folder = data_folder + 'DataFrames/'\n",
    "traits_folder = data_folder + 'Traits/'\n",
    "survey_folder = 'Data/'\n",
    "\n",
    "\n",
    "results_folder = 'SurveyResults/Mistral/'\n",
    "os.makedirs(results_folder, exist_ok = True)\n",
    "\n",
    "sentence_fol = results_folder + '/per_sentence'\n",
    "os.makedirs(sentence_fol, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f946a",
   "metadata": {},
   "source": [
    "Read: 1) The Traits for the fiven name. 2) The sentences/paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the traits. we are going to use this to query the LLM\n",
    "with open(F\"{traits_folder}Caribbean.json\", 'r') as f:\n",
    "  traits_dict_caribbean = json.load(f)\n",
    "\n",
    "with open(F\"{traits_folder}West.json\", 'r') as f:\n",
    "  traits_dict_pnet = json.load(f)\n",
    "\n",
    "with open(F\"{traits_folder}Palm.json\", 'r') as f:\n",
    "  traits_dict_palm = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc32c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(traits_dict)\n",
    "for key in traits_dict_pnet:\n",
    "    print('{}:{}\\n'.format(key, traits_dict_pnet[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read the survey data\n",
    "surveys = pd.read_csv(survey_folder+'answers_surveys.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = surveys['Species'].values\n",
    "main_traits = surveys['Main Trait'].values\n",
    "sentences = surveys['Sentence'].values\n",
    "dataset = surveys['Dataset'].values\n",
    "results = surveys['Result'].values\n",
    "subtraits = surveys['GT Sub Traits'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991f37e",
   "metadata": {},
   "source": [
    "### Check that all traits in the file are found in the GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traits_dict_caribbean.keys())\n",
    "print(traits_dict_pnet.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa44ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, key in enumerate(main_traits):\n",
    "    key = key.replace(' /','').strip()\n",
    "    #if dataset[idx] == 'Palm':\n",
    "    #    continue\n",
    "    if key not in traits_dict_caribbean and key not in traits_dict_pnet and key not in traits_dict_palm:\n",
    "        print('Error trait: {} dataset: {}', key, dataset[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556fb34",
   "metadata": {},
   "source": [
    "## Combine sentences and traits. \n",
    "Iterate the given configurations and ask gpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c63fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all the species\n",
    "results = []\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print('Cur Sentence Num: {}/{}'.format(idx, len(sentences)))\n",
    "    \n",
    "    if idx not in [188]:\n",
    "        continue\n",
    "        \n",
    "    #if dataset[idx] == 'Palm':\n",
    "    #    continue\n",
    "    \n",
    "    # create the folder for the species. replace blanks with underscores to avoid potential issues\n",
    "    #res_folder = results_folder + '/sentence_num_{}'.format(idx)\n",
    "    #os.makedirs(res_folder, exist_ok = True)\n",
    "    \n",
    "    #idx = 2\n",
    "    #sentence = sentences[idx]\n",
    "    # get the current info for this question \n",
    "    main_trait = main_traits[idx]\n",
    "    subtrait = eval(subtraits[idx])\n",
    "    spec = species[idx]\n",
    "    \n",
    "\n",
    "    trait_list = [main_trait]\n",
    "    pos_traits = \"{\" + '\\\"{}\\\": {} '.format(main_trait.capitalize(), subtrait) + '}'\n",
    "    \n",
    "    \n",
    "    text = 'We are interested in obtaining botanical trait information about the species {}.\\n\\n'.format(spec)\n",
    "    text += 'We will provide an input text with botanical descriptions,'\\\n",
    "            'followed by a dictionary where each key \\'name\\' represents a trait name, '\\\n",
    "            'referring to specific organ or other element of the plant, and is associated to a list '\\\n",
    "            'with all possible trait values for that trait, [\\'value_1\\', \\'value_2\\', ..., \\'value_n\\'].\\n\\n'\n",
    "    \n",
    "    text += 'Input text:\\n'\n",
    "    text += sentence +'\\n\\n'\n",
    "    \n",
    "    text += 'Initial dictionary of traits with all possible values:\\n'\n",
    "    text += pos_traits +'\\n\\n'\n",
    "    \n",
    "    text += 'Turn each string s in the list of values in the dictionary into a sublist (s,b), where b is a binary number,'\\\n",
    "             'either 0 or 1, indicating whether there is strong evidence for value s in the input text. '\n",
    "    text+= 'Double check that \\'value_i\\' is reported referring to trait \\'name\\' in the text, '\\\n",
    "            'and not to a different trait. Always set \\'b\\' to \\'0\\' if you are not 100% sure about '\\\n",
    "            'the association. Do not add new trait values and do not modify the initial ones.Return the dictionary of traits and sublists of (value, evidence) containing ALL POSSIBLE NAMES AND (values, evidence) tuples.\\n\\n'\n",
    "    text += 'Output only a dictionary in JSON format, no other text at all.\\n\\n'\n",
    "    \n",
    "    \n",
    "    messages = [ChatMessage(role=\"user\", content = text)]\n",
    "    \n",
    "    retries = 5\n",
    "    while retries>0:\n",
    "        try:\n",
    "\n",
    "            chat_response = client.chat(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "            )\n",
    "            content = chat_response.choices[0].message.content\n",
    "            content_as_json = json.loads(content)\n",
    "            \n",
    "            retries = 0.\n",
    "            break\n",
    "        except (Exception, JSONDecodeError) as e:\n",
    "            if e:\n",
    "                print('Some Kind of Error, {}'.format(e))\n",
    "                retries -= 1\n",
    "                time.sleep(5)\n",
    "\n",
    "\n",
    "    \n",
    "    results.append(content)\n",
    "    \n",
    "    \n",
    "    with open('{}/sentence_{}_prompt_and_response.txt'.format(sentence_fol, idx), 'w') as f:\n",
    "        f.write('{}'.format(text))\n",
    "        f.write('\\n\\n{}'.format(content))\n",
    "\n",
    "with open(results_folder + '/responses_mistral_surveys.txt', 'w') as f:\n",
    "    for res in results:\n",
    "        f.write(res + '\\n')\n",
    "with open(results_folder + '/responses_mistral_surveys_cleaned.txt', 'w') as f:\n",
    "    for res in results:\n",
    "        f.write(res.replace('\\n','') + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_folder + '/responses_mistral_surveys_cleaned.txt', 'w') as f:\n",
    "    for res in results:\n",
    "        res = res.split('Note')[0]\n",
    "        res = res.split('Explanation')[0]\n",
    "        f.write(res.replace('\\n','') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481f0c",
   "metadata": {},
   "source": [
    "Check some stuff if the previous fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21397d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_as_json = json.loads(content)\n",
    "print(content_as_json)\n",
    "print(key)\n",
    "print(eval(content_as_json[key][0]))\n",
    "print(eval(content_as_json[key][0])[0])\n",
    "print(type(eval(content_as_json[key][0])[1]))\n",
    "\n",
    "trait_list\n",
    "content_as_json['Bark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36acf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')    \n",
    "response_final = []\n",
    "for i in range(1216):\n",
    "    with open(results_folder + '/per_sentence/sentence_{}_prompt_and_response.txt'.format(i), 'r') as f:\n",
    "        resp = f.read()\n",
    "        \n",
    "    patt = pattern.findall(resp)[-1]\n",
    "    response_final.append(patt)\n",
    "with open(results_folder + '/responses_mistral_surveys_cleaned.txt', 'w') as f:\n",
    "    for res in response_final:\n",
    "        f.write(res.replace('\\n','') + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
