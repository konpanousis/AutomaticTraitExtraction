{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87b5774-7742-448d-8f6a-dfd0543f052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "# set a fixed seed\n",
    "import random\n",
    "random.seed(333)\n",
    "\n",
    "# the dotenv contains all the credentials for queries and prompts\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "base_folder = 'Data'\n",
    "dataset_name = 'example'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e309fdb",
   "metadata": {},
   "source": [
    "## Set up the species, traits and trait values of interest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb787a01-f9ff-417f-ab28-ed68dc35b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short example of how to input the species and trait/trait-value vocabulary\n",
    "species_to_query = ['Ceroxylon peruvianum', 'Calamus australis']\n",
    "\n",
    "traits_dict =  {\n",
    "    'Fruit Colour': ['black', 'blue', 'brown', 'green', 'grey', 'ivory', 'orange', 'pink', 'purple', 'red', 'white', 'yellow'],\n",
    "    'Flower Colour': ['black', 'blue', 'brown', 'green', 'grey', 'ivory', 'orange', 'pink', 'purple', 'red', 'white', 'yellow'],\n",
    "    'Crown layer': ['both', 'canopy', 'understorey'],\n",
    "    'Fruit Size': ['large', 'small'],\n",
    "    'Fruit Shape': ['ellipsoid', 'elongate', 'fusiform', 'globose', 'ovoid', 'pyramidal', 'rounded'],\n",
    "    'Conspicuousness': ['conspicuous', 'cryptic']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An alternative example, using a .xlsx file containing the species and traits of interest\n",
    "# Load the .xlsx file\n",
    "file_path = 'WFO_Solanum_description.xlsx'  # Replace with the path to your .xlsx file\n",
    "\n",
    "# Read all sheets from the file into a dictionary of dataframes\n",
    "# Each sheet's name or index will be the key in the dictionary\n",
    "sheets_dict = pd.read_excel(file_path, sheet_name=None, index_col=None, header=None)  # None loads all sheets\n",
    "\n",
    "# Access individual sheets by name or by index\n",
    "sheet1 = sheets_dict[list(sheets_dict.keys())[0]]  # First sheet by index\n",
    "sheet2 = sheets_dict[list(sheets_dict.keys())[1]]  # Second sheet by index\n",
    "sheet3 = sheets_dict[list(sheets_dict.keys())[2]]  # Third sheet by index\n",
    "\n",
    "def keep_first_two_words(text):\n",
    "    words = text.split()  # Split the string into words\n",
    "    first_two = words[:2]  # Get the first two words\n",
    "    return ' '.join(first_two)  # Join them back into a string\n",
    "\n",
    "species_to_query = list(sheet1.iloc[0:10,0])\n",
    "for i in range(len(species_to_query)):\n",
    "    species_to_query[i] = keep_first_two_words(species_to_query[i])\n",
    "\n",
    "species_to_query = [\n",
    "    'Solanum lycopersicum',\n",
    "'Solanum dulcamara',\n",
    "'Solanum nigrum',\n",
    "'Solanum tuberosum',\n",
    "'Solanum laxum',\n",
    "'Solanum americanum',\n",
    "'Solanum melongena',\n",
    "'Solanum pseudocapsicum',\n",
    "'Solanum laciniatum',\n",
    "'Solanum villosum' \n",
    "]\n",
    "\n",
    "def clean_string(s):\n",
    "    if isinstance(s, str):\n",
    "        # Replace any special characters (like \\n, \\r, etc.) using regex\n",
    "        return re.sub(r'[\\n\\r]', '', s).strip()  # Add more characters in the regex as needed\n",
    "    return s\n",
    "\n",
    "# Initialize the dictionary to hold the names and associated surnames\n",
    "traits_dict = {}\n",
    "current_name = None\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in sheet3.iterrows():\n",
    "    name = clean_string(row[0])  # First column (name or index)\n",
    "    surname = row[1]  # Second column (surname or NaN)\n",
    "    \n",
    "    if pd.isna(surname) and isinstance(name, str) and name != ' ':  # Name row\n",
    "        current_name = name\n",
    "        traits_dict[current_name] = []\n",
    "    elif not pd.isna(surname) and current_name is not None:  # Surname row\n",
    "        traits_dict[current_name].append(surname)\n",
    "\n",
    "del traits_dict['Cultivated and non-native distribution']\n",
    "del traits_dict['Major clade (group)']\n",
    "del traits_dict['Geographic distribution']\n",
    "del traits_dict['Minor clade (group)']\n",
    "del traits_dict['Anther length']\n",
    "del traits_dict['Filament length']\n",
    "del traits_dict['']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623f17e-78c4-4fc3-9c0b-c3d96213a5bf",
   "metadata": {},
   "source": [
    "## Custom Search Engine Setup\n",
    "---\n",
    "\n",
    "To retrieve the corresponding URLs for the species that we are goind to check, we are using the Google Custom Search API. To this end, we must first assign the appropriate credentials and then construct a function to define a custom service engine. See more about the Custom Search API and how to set it up here: [Custom Search API](https://developers.google.com/custom-search/v1/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959333a9-997b-4e7b-815a-2cdc2ef5a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the google API credentials \n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "google_api_key = os.environ['GOOGLE_API']\n",
    "cse_id = os.environ['GOOGLE_CSE']\n",
    "\n",
    "# create a search query\n",
    "def google_search(exact_term, other_search_term, api_key, cse_id, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a search link for the custom Google search.\n",
    "    \"\"\"\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(exactTerms=exact_term, q=other_search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res['items']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f4fe6-7806-405d-baa0-eed07faba354",
   "metadata": {},
   "source": [
    "## Query the Custom Search Engine\n",
    "---\n",
    "\n",
    "In the following, we define the search terms that we will search along with the names of the species defined before. Then, we utilize the **google_search** function defined in the previous blocks to retrieve the results from the custom search engine.\n",
    "\n",
    "Throughout the notebook, we store all the intermediate data that emerge using the *pickle* python package. In the following, we are storing the richer retrieved urls that include the site title and other information (*species_urls_full*) and just the urls (*species_urls*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fdfa09-ef6c-4a78-b489-f3bb350556b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# the search terms to use along with the selected species\n",
    "search_terms = [\"description\",\"characteristics\"]\n",
    "\n",
    "# save the retrieved urls and links\n",
    "species_urls = collections.defaultdict(list)\n",
    "species_urls_full = collections.defaultdict(list)\n",
    "\n",
    "# parse all the species and search terms\n",
    "for species in tqdm(species_to_query):\n",
    "    for search_term in search_terms:\n",
    "        # constructs the query, e.g., Archontophoenix maxima description.\n",
    "        #query = F'{species} {search_term}'\n",
    "        exact_term = F'{species}'\n",
    "        other_search_term = F'{search_term}'\n",
    "\n",
    "        # Search results \n",
    "        search_results = google_search(exact_term, other_search_term, api_key=google_api_key, cse_id=cse_id)\n",
    "        \n",
    "        # Record the google Urls\n",
    "        species_urls_full[species] = search_results\n",
    "        \n",
    "        # Recoerd just the links\n",
    "        for result in search_results:\n",
    "            species_urls[species].append(result['link'])\n",
    "\n",
    "\n",
    "#save the results for all the species to files\n",
    "path_to_save = f'{base_folder}/Search_Query_Results/{dataset_name}_dataset/'\n",
    "\n",
    "#create the folder if it does not exist\n",
    "os.makedirs(path_to_save, exist_ok = True)\n",
    "\n",
    "# specific paths for urls and links\n",
    "url_path = f'{path_to_save}urls.pkl'\n",
    "full_url_path = f'{path_to_save}full_urls.pkl'\n",
    "\n",
    "# now save the data to respective files \n",
    "with open(url_path, 'wb') as f:\n",
    "    pickle.dump(species_urls, f)\n",
    "\n",
    "with open(full_url_path, 'wb') as f:\n",
    "    pickle.dump(species_urls_full, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65dc904-3305-40aa-95ae-c6a7f5b90d9a",
   "metadata": {},
   "source": [
    "### URL Processing: Converting results to plain text\n",
    "---\n",
    "\n",
    "We need to extract the text from the links in order to be able to process them and use them for prompting. We use the *requests* and *bs4* packages to this end in the following snippet to define a function to extract paragraphs from urls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffe23fc-24e6-4cb6-9d09-17ca43b5ff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Species: 100%|██████████| 10/10 [02:40<00:00, 16.03s/it]                             \n"
     ]
    }
   ],
   "source": [
    "# for parsing links \n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_paragraphs_from_urls(url_dict):\n",
    "\n",
    "    # Initialise the dictionary that will contain the plain text for each species\n",
    "    text_dict = collections.defaultdict(list)\n",
    "\n",
    "    # Loop over species and URLS to extract the text\n",
    "    for (species, urls) in tqdm(url_dict.items(), desc=\"Species\", leave=True, position=0):\n",
    "        for url in tqdm(urls, desc=f\"URLs Species: {species}\", leave=False, position=0):\n",
    "\n",
    "            try:\n",
    "                session = requests.Session()\n",
    "                retry = Retry(total=1,\n",
    "                              connect=1, \n",
    "                              backoff_factor=0.5)\n",
    "                adapter = HTTPAdapter(max_retries=retry)\n",
    "                session.mount('http://', adapter)\n",
    "                session.mount('https://', adapter)\n",
    "                response = session.get(url, timeout=3)\n",
    "                \n",
    "                \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                paragraphs = soup.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    text_dict[species].append(paragraph.text)\n",
    "\n",
    "            except:\n",
    "                text_dict[species].append(\"Invalid URL\")\n",
    "\n",
    "    return text_dict\n",
    "\n",
    "# call the extract paragraph function and save the results \n",
    "species_text = extract_paragraphs_from_urls(species_urls)\n",
    "\n",
    "with open(F\"{path_to_save}paragraphs.pkl\", 'wb') as f:\n",
    "    pickle.dump(species_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6558466-45c5-478b-a7bc-340b7bbf812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### print some of the paragraphs to see if everything is ok. \n",
    "species_text[species_to_query[-1]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d069f6-e34a-4a2e-9613-75334a45b6f6",
   "metadata": {},
   "source": [
    "### Text Processing: Filter and Clean\n",
    "---\n",
    "\n",
    "The extracted paragraphs are highly \"noisy\": they contain special and irrelevant characters, while some others cannot be encoded correctly or they are repeated, e.g., whitespace characters. The following snippet used the *re* package along with some regular expressions to clean the text. After cleaning is performed, we remove any potential duplicate entries (by converting the list of paragraphs to a python set) and the return the valid pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc315595-46c5-47d9-983c-3a1526957684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    }
   ],
   "source": [
    "def regex_cleaner(string):\n",
    "    # Define a list of regular expression patterns and their replacements\n",
    "    cleaners = [\n",
    "        # Replace multiple consecutive whitespace characters (spaces, tabs, newlines) with a single space character\n",
    "        (\"\\s+\", \" \"),\n",
    "        # Replace multiple consecutive newline characters with a single newline character\n",
    "        (\"\\n+\", \"\\n\"),\n",
    "        # Replace multiple consecutive tab characters with a single tab character\n",
    "        (\"\\t+\", \"\\t\"),\n",
    "        # remove non-alphanumeric characters\n",
    "        (r'[^\\w\\s/]', ''),\n",
    "        # replace malformed characters,\n",
    "        ('Â',''),\n",
    "        ('â€“', '-'),\n",
    "        ('·','.'),\n",
    "        ('Ã','x'),\n",
    "        (u'\\xa0', u' '),\n",
    "        ('â€‰', ''),\n",
    "        ('â€', '-'),\n",
    "        ('x©', 'e'),\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Apply each regular expression pattern and its replacement to the input string\n",
    "    for (cleaner, replacement) in cleaners:\n",
    "        string = re.sub(cleaner, replacement, string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return string.strip()\n",
    "\n",
    "def filter_species_dict(text_dict):\n",
    "    \"\"\"\n",
    "    Filters the descriptions in a dictionary of species, removing invalid text and duplicates.\n",
    "\n",
    "    Args:\n",
    "        text_dict (dict): A dictionary where the keys are the species names and the values are lists of text descriptions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A filtered dictionary where the keys are the species names and the values are lists of valid and unique text descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    valid_species_dict = {}\n",
    "\n",
    "    # Loop through each species and its descriptions in the dictionary\n",
    "    for idx, (species, descriptions) in enumerate(tqdm(text_dict.items(), leave=False, position=0)):\n",
    "        # Create a progress bar for the species\n",
    "        species_description = f\"{idx} {species}\"\n",
    "        species_pbar = tqdm(descriptions, leave=False, position=1, desc=species_description)\n",
    "\n",
    "        valid_descriptions = []\n",
    "        # Loop through each description for the species\n",
    "        for description in species_pbar:\n",
    "            # Clean the description using regex_cleaner\n",
    "            cleaned_description = regex_cleaner(description)\n",
    "           \n",
    "            if len(cleaned_description) < 10000 and len(cleaned_description) > 1:\n",
    "                valid_descriptions.append(cleaned_description)\n",
    "\n",
    "        # Remove any duplicate descriptions in the list\n",
    "        valid_descriptions = list(set(valid_descriptions))\n",
    "        # Add the valid descriptions for the species to the valid_species_dict\n",
    "        valid_species_dict[species] = valid_descriptions\n",
    "\n",
    "    return valid_species_dict\n",
    "\n",
    "# now clean the text extracted from the urls and save the results\n",
    "species_text_cleaned = filter_species_dict(species_text)\n",
    "\n",
    "with open(f\"{path_to_save}paragraphs_cleaned.pkl\", 'wb') as f:\n",
    "    pickle.dump(species_text_cleaned, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814e1a3-b73e-44a8-b763-a9397bfad971",
   "metadata": {},
   "source": [
    "### Text Processing: Cleaned Text to Descriptions Classification\n",
    "---\n",
    "\n",
    "We now have a useful list of texts for each species from the extracted information from the web. However, it is highly likely that most of the extracted and cleaned parts do not contain any useful information for the respective species. To this end, we trained a Description Classifier; this model takes as input a piece of text and classifies the text as being a description or not. We retain the ones that are classified as descriptions while dicarding the rest. \n",
    "\n",
    "The model is based on the BERT model. We first import all necessary libraries and define the essential components; then we load the trained checkpoint and perform classification.\n",
    "\n",
    "Requires: python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62eae3-9aaf-4e54-9fb3-9aa58a1dfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import warnings\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Load some utilities\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "# define the base bert model that we finetune for description classification\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        # Distil Bert model\n",
    "        self.bert = bert\n",
    "        ## Additional layers\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # Dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        # Dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        # Softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, **kwargs):\n",
    "\n",
    "        #pass the inputs to the model BERT  \n",
    "        cls_hs = self.bert(**kwargs)\n",
    "        hidden_state = cls_hs.last_hidden_state\n",
    "        pooler = hidden_state[:, 0]\n",
    "\n",
    "        # dense layer 1        \n",
    "        x = self.fc1(pooler)\n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Drop out\n",
    "        x = self.dropout(x)\n",
    "        # dense layer 2\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Define an instance of the BERT model and load the weights\n",
    "model = BERT(bert).to(device)\n",
    "\n",
    "modelname = \"saved_weights_BERT_description_classifier.pt\"\n",
    "location = \"models/\"\n",
    "\n",
    "model_save_name = modelname\n",
    "path = location + model_save_name\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12a22f-61f6-4b6c-a818-a2990a337982",
   "metadata": {},
   "source": [
    "### Classification Function and Processing Functions\n",
    "---\n",
    "\n",
    "Having defined the description classifier, we now construct the appropriate functions for classifying the cleaned texts to descriptions. Instead of directly classifying the whole paragraph, it is first split into sentences using the *SpaCy* package. We store both individual sentences that were classified as descriptions, as well as new paragraphs containing only the descriptive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5581601-5f18-4f9c-a102-2f1bb082547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paragraphs Species: Solanum lycopersicum:   0%|          | 0/439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Species: 100%|██████████| 10/10 [02:41<00:00, 16.15s/it]                                     \n"
     ]
    }
   ],
   "source": [
    "def classify_text(span, model, truncation=True, device = 'cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses a trained bert classifier to see if a span\n",
    "    belongs to a species description or not.\n",
    "    \"\"\"\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(span, return_tensors=\"pt\", truncation=truncation).to(device)\n",
    "        # Predict class\n",
    "        outputs = model(**inputs)\n",
    "        # Get prediction values\n",
    "        exps = outputs.softmax(1)\n",
    "        # Get class\n",
    "        span_class = (exps[:,1]>0.6).item()\n",
    "\n",
    "        return span_class\n",
    "\n",
    "\n",
    "def paragraph_to_descriptions(paragraph_dict, device = 'cpu'):\n",
    "    \"\"\"Converts a dictionary of paragraphs to descriptions for each species.\n",
    "\n",
    "    Args:\n",
    "        paragraph_dict (dict): A dictionary where keys are species and values\n",
    "            are lists of paragraphs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, List[str]], Dict[str, List[str]]]: A tuple of two \n",
    "        dictionaries. The first dictionary contains the descriptions for each \n",
    "        species where each description is a concatenation of several sentences.\n",
    "        The second dictionary contains the sentences for each species that \n",
    "        passed a classification check.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries\n",
    "    description_paragraph_dict = collections.defaultdict(list)\n",
    "    description_sentence_dict = collections.defaultdict(list)\n",
    "\n",
    "    # Process each species\n",
    "    for (species, paragraphs) in tqdm(paragraph_dict.items(), desc=\"Species\", leave=True, position=0):\n",
    "\n",
    "        # Process each paragraph\n",
    "        for paragraph in tqdm(paragraphs, desc=f\"Paragraphs Species: {species}\", leave=False, position=0):\n",
    "\n",
    "            # Ignore very long paragraphs\n",
    "            if len(paragraph) > 80000:\n",
    "                continue\n",
    "            \n",
    "            # Parse paragraph with spaCy\n",
    "            doc = nlp(paragraph)\n",
    "\n",
    "            # Store sentences that pass classification check\n",
    "            new_paragraph = []\n",
    "            for sent in doc.sents:\n",
    "                if classify_text(sent.text, model=model, device = device):\n",
    "                    description_sentence_dict[species].append(sent.text)\n",
    "                    new_paragraph.append(sent.text)\n",
    "\n",
    "            # Store paragraph if it has valid sentences\n",
    "            if new_paragraph:\n",
    "                description_paragraph_dict[species].append(' '.join(new_paragraph))\n",
    "\n",
    "    return description_paragraph_dict, description_sentence_dict\n",
    "\n",
    "# classify sentences and save\n",
    "description_sentence_dict, description_paragraph = paragraph_to_descriptions(species_text_cleaned, device = device)\n",
    "\n",
    "description_sentence_path = f'{path_to_save}description_sentences.pkl'\n",
    "description_paragraphs_path = f'{path_to_save}description_paragraphs.pkl'\n",
    "\n",
    "with open(description_sentence_path, 'wb') as f:\n",
    "    pickle.dump(description_sentence_dict, f) \n",
    "\n",
    "with open(description_paragraphs_path, 'wb') as f:\n",
    "    pickle.dump(description_paragraph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=4, width=500)\n",
    "pp.pprint(description_sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81990fe9-eca6-4f7c-9b6e-f5414f92f795",
   "metadata": {},
   "source": [
    "### LLM Prompting\n",
    "---\n",
    "\n",
    "We now have the processed and classified text for each species. With this data at hand, we can prompt an LLM to explore if we can find information about species traits. First, we need to define the LLM client, e.g. MistralClient, load/set the traits that we want to epxlore and then use the sentences from the previous snippet to construct a prompt that we feed to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79f07aa-f9b5-455d-9e18-3d026db32601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up the mistral/LLM API credentials\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-medium-latest\"\n",
    "\n",
    "client = MistralClient(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'Solanum10/'\n",
    "\n",
    "# define the name of the dataset \n",
    "results_folder = f'{data_folder}Prompt_Results/Mistral/{dataset_name}_dataset'\n",
    "os.makedirs(results_folder, exist_ok = True)\n",
    "\n",
    "# also save the traits dictionary in a text file\n",
    "with open(f'{results_folder}/traits.txt', 'w') as f:\n",
    "    for tname in traits_dict:\n",
    "        f.write('{}: {}\\n'.format(tname, traits_dict[tname]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f74b9e-847a-4825-9eb5-ccaae4e77189",
   "metadata": {},
   "source": [
    "### LLM Prompting: Main Loop\n",
    "---\n",
    "\n",
    "Having loaded the traits, we proceed with the core loop that is responsible for the prompting of the LLM. For each species and its sentences, we construct an appropriate prompt (see the *text* variable below) that is sent to the LLM via the client chat function. We record the response of the LLM, making sure that it is in an appropriate form (JSON) before proceeding; otherwise we re=prompt the LLM. We save the results in human-readable form, i.e., plain text format. Then, we can proceed with the aggregation of the information for all the species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traits are in traits\n",
    "traits_names = list(traits_dict.keys())\n",
    "traits_names_cap = [ tr.capitalize() for tr in traits_dict.keys() ]\n",
    "    \n",
    "# parse all the species\n",
    "for idx, species in enumerate(description_sentence_dict):\n",
    "    \n",
    "    \n",
    "    print('Cur Species Num: {}/{} Name: {}'.format(idx+1, len(description_sentence_dict), species))\n",
    "\n",
    "    \n",
    "    # create the folder for the species. replace blanks with underscores to avoid potential issues\n",
    "    species_folder = results_folder + '/{}'.format(species.replace(' ', '_'))\n",
    "    os.makedirs(species_folder, exist_ok = True)\n",
    "    \n",
    "    \n",
    "    # this is the list with all the sentences, we are gonna iterate and combine.\n",
    "    sentences_ = description_sentence_dict[species]\n",
    "    \n",
    "    #print(sentences_)\n",
    "    # save cleaned sentences in a txt format\n",
    "    with open(f'{species_folder}/sentences_cleaned.txt', 'w') as f:\n",
    "        for sent in sentences_:\n",
    "            f.write(sent+'\\n')\n",
    "        \n",
    "    # all the reponses and contents only for the species \n",
    "    responses_full = []\n",
    "    contents = []\n",
    "    llm_dict_traits = {}\n",
    "    \n",
    "    cur_paragraph = '\\n'.join(sentences_)\n",
    "    \n",
    "    pos_traits = '{'\n",
    "    for j in range(0, len(traits_names)):\n",
    "        pos_traits += '\\\"{}\\\": {}, '.format(traits_names[j].capitalize(), traits_dict[traits_names[j]]                                    )\n",
    "        # until the third to last element to remove comma and space...\n",
    "    pos_traits = pos_traits[:-2] + '}'\n",
    "     \n",
    "    \n",
    "    text = 'We are interested in obtaining botanical trait information about the species {}.\\n\\n'.format(species)\n",
    "    text += 'We will provide an input text with botanical descriptions,'\\\n",
    "            'followed by a dictionary where each key \\'name\\' represents a trait name, '\\\n",
    "            'referring to specific organ or other element of the plant, and is associated to a list '\\\n",
    "            'with all possible trait values for that trait, [\\'value_1\\', \\'value_2\\', ..., \\'value_n\\'].\\n\\n'\n",
    "    \n",
    "    text += 'Input text:\\n'\n",
    "    text += cur_paragraph +'\\n\\n'\n",
    "    \n",
    "    text += 'Initial dictionary of traits with all possible values:\\n'\n",
    "    text += pos_traits +'\\n\\n'\n",
    "    \n",
    "    text += 'Turn each string s in the list of values in the dictionary into a sublist (s,b), where b is a binary number,'\\\n",
    "             'either 0 or 1, indicating whether there is strong evidence for value s in the input text. '\n",
    "    text+= 'Double check that \\'value_i\\' is reported referring to trait \\'name\\' in the text, '\\\n",
    "            'and not to a different trait. Set \\'b\\' to \\'0\\' if you are not sure about '\\\n",
    "            'the association. Do not modify the initial trait names and trait values, and do not add other traits than those provided in the dictionary. '\\\n",
    "            'Return the dictionary of traits and sublists of (value, evidence) containing all possible names and (value, evidence) tuples.\\n\\n'\n",
    "    text += 'Output first a dictionary in JSON format, followed by a very short textual explanation for each positive response.\\n\\n'\n",
    "    \n",
    "\n",
    "    cur_path = '{}/results/'.format(species_folder)\n",
    "\n",
    "    os.makedirs(cur_path, exist_ok = True)\n",
    "  \n",
    "    messages = [ChatMessage(role=\"user\", content = text)]\n",
    "    \n",
    "    retries = 3\n",
    "    while retries>0:\n",
    "        try:\n",
    "\n",
    "            chat_response = client.chat(\n",
    "                model=model,\n",
    "                #response_format={\"type\": \"json_object\"},\n",
    "                messages=messages,\n",
    "            )\n",
    "            chat_response = chat_response.choices[0].message.content\n",
    "            content = re.search(r'{.*}', chat_response, re.DOTALL).group()\n",
    "            content_as_json = json.loads(content)\n",
    "            \n",
    "            retries = 0.\n",
    "            break\n",
    "        except (Exception, JSONDecodeError) as e:\n",
    "            if e:\n",
    "                print('Some Kind of Error, {}'.format(e))\n",
    "                retries -= 1\n",
    "                time.sleep(5)\n",
    "\n",
    "    \n",
    "    with open('{}/mistral_response_full.txt'.format(cur_path), 'w') as f:\n",
    "        f.write(str(chat_response))\n",
    "    with open('{}/mistral_response_content_only.txt'.format(cur_path), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "    with open('{}/mistral_sent_info_and_content.txt'.format(cur_path), 'w') as f:\n",
    "        f.write('{}\\n\\n{}'.format(text, content))\n",
    "\n",
    "    responses_full.append(str(chat_response))\n",
    "    contents.append(content)\n",
    "\n",
    "\n",
    "    with open('{}/responses_all.txt'.format(species_folder), 'w') as f:\n",
    "        for resp in responses_full:\n",
    "            f.write(resp + '\\n\\n')\n",
    "    with open('{}/contents_all.txt'.format(species_folder), 'w') as f:\n",
    "        for cont in contents:\n",
    "            f.write(cont + '\\n\\n')\n",
    "\n",
    "print(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad3469-b0d0-4bca-8933-a44c3e12045b",
   "metadata": {},
   "source": [
    "### Post Processing \n",
    "---\n",
    "\n",
    "All the responses of the LLM for each species are saved in their corresponding folders. All that is left is to aggregate the information from the individual species and output the results. We use the functions defined in the *aggregate_traits.py* file, found in the notebook's folder, and specifically the *post_processing* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f61a3-4fb9-4358-b671-86d3d5ba61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregate_traits import post_processing\n",
    "post_processing(traits_dict, species_to_query,results_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
